---
layout: page
title: "[ISLR] 데이터마이닝 솔루션 (13)"
description:
headline: 
categories: data-science
tags: [data-science]
comments: true
published: true
---

```r
# library load
library(ISLR)
library(class)
library(pROC)
library(boot)
```

## 5.5 (a) fit logistic regression model
분류 모형평가 4장에서, 로지스틱 회귀를 사용해 income 과 balance을 가지고 default 확률을 예측했다. validation set approach를 사용하여 로지스틱 회귀에 대한 test error를 추정해보자. (Do not forget to set a random seed before beginning your analysis.)
```r
set.seed(1)
data(Default)
fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial")
summary(fit.glm)
## 
## Call:
## glm(formula = default ~ income + balance, family = "binomial", 
##     data = Default)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4725  -0.1444  -0.0574  -0.0211   3.7245  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(>|z|)    
## (Intercept) -1.154e+01  4.348e-01 -26.545  < 2e-16 ***
## income       2.081e-05  4.985e-06   4.174 2.99e-05 ***
## balance      5.647e-03  2.274e-04  24.836  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2920.6  on 9999  degrees of freedom
## Residual deviance: 1579.0  on 9997  degrees of freedom
## AIC: 1585
## 
## Number of Fisher Scoring iterations: 8
```

## 5.5 (b) validation set approach => test error
```r
#  학습세트와 검증세트를 나누는 비율에 따른 cv를 함수로
cv <- function(ratio) {
#i)
n = nrow(Default)
ind.shuff <- sample(1:n, size = n)
#----------ratio----------#
n.tr <- floor(n * ratio)
n.val <- n - n.tr
#-------------------------#
train <- Default[ind.shuff[1:n.tr],]
val <- Default[ind.shuff[(n.tr+1):n],]

#ii)
fit.glm.train <- glm(default ~ income + balance, data = train, family = "binomial")
print(summary(fit.glm.train))

#iii)
x.val = val[c("income", "balance")]

y.val.hat <- as.matrix(cbind(1, x.val)) %*% as.vector(coef(fit.glm.train))
fit.prob <- exp(y.val.hat) / (1 + exp(y.val.hat))

y.val.class <- ifelse(fit.prob > 0.5, "Yes", "No")

#+iv)
mat <- table(val$default, y.val.class)
print(mat) # 정오행렬

result <- numeric(5)

result[1] <- (mat[2]+mat[3]) / sum(mat) #오분류율
result[2] <- mat[4] / (mat[2]+mat[4]) #민감도
result[3] <- mat[1] / (mat[1]+mat[3]) #특이도
result[4] <- mat[4] / (mat[3]+mat[4]) #정밀도
result[5] <- mat[4] / (mat[2]+mat[4]) #재현율

names(result) <- c("오분류율","민감도","특이도","정밀도","재현율") 
print(result)

#+v) (iv)에서는 cutoff를 0.5로 정하여 오류지표를 계산하였다. 
#cutoff를 정하지 않은 상태를 가정하고, ROC 곡선을 그린 후 AUROC를 계산하세요.

# pROC패키지 사용시
r <- roc(val$default, as.vector(fit.prob))
plot.roc(r)
print(auc(r)) #0.9506

# ROC곡선 하드코딩; 하드코딩이라 중간에 직선은 이쁘게 봐주세여.. 0.6쯤부터는 값이 나오지 않으므로 이어지질 않더라구요. (roc 패키지는 알아서 해주는 부분..)

cut <- seq(1,0, by = -0.001)
roccurve <- matrix(0, ncol = 2, nrow = length(cut))
for (i in 1:length(cut)) {
  y.val.hat.res <- ifelse(fit.prob > cut[i], "Yes", "No")
  t <- table(val$default, y.val.hat.res)
  if (!("No" %in% y.val.hat.res)) { roccurve[i,] <- c(0, 0); next;}
  if (!("Yes" %in% y.val.hat.res)) { roccurve[i,] <- c(1, 1); next;}
  t <- table(val$default, y.val.hat.res)
  specificity <- t[1] / (t[1] + t[3])
  sensitivity <- t[4] / (t[2] + t[4])
  roccurve[i,] <- c(1-specificity, sensitivity)
}
k <- data.frame(seq(roccurve[1000,1],1,by = 0.01),1) # 중도에 끊기는 부분부터, 1로 채워줌
roccurve <- data.frame(roccurve)
names(k) <- names(roccurve)
roccurve <- rbind(roccurve, k)
plot(roccurve[,1], roccurve[,2], type = "l", col = "skyblue", lwd = 4, xlab="1-특이도", ylab="민감도", main=" >하드코딩< ROC 커브 ")

}

cv(0.5) # ratio=0.5 => by validation set approach, 나이브하게
```

```r
## 
## Call:
## glm(formula = default ~ income + balance, family = "binomial", 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.5830  -0.1428  -0.0573  -0.0213   3.3395  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(>|z|)    
## (Intercept) -1.194e+01  6.178e-01 -19.333  < 2e-16 ***
## income       3.262e-05  7.024e-06   4.644 3.41e-06 ***
## balance      5.689e-03  3.158e-04  18.014  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1523.8  on 4999  degrees of freedom
## Residual deviance:  803.3  on 4997  degrees of freedom
## AIC: 809.3
## 
## Number of Fisher Scoring iterations: 8
## 
##      y.val.class
##         No  Yes
##   No  4824   19
##   Yes  108   49
##  오분류율    민감도    특이도    정밀도    재현율 
## 0.0254000 0.3121019 0.9960768 0.7205882 0.3121019
```
<img src="/images/2020-09/islr13-1.png"  width="700" height="370">
- Area under the curve: 0.9419

<img src="/images/2020-09/islr13-2.png"  width="700" height="370">

## 5.5 (b)의 수정/추가사항을 모두 반영하여 답하세요.
```r
#options(warn = 0)
par(mfrow=c(3,2))
cv(0.9) # tr:val = 9:1
## 
## Call:
## glm(formula = default ~ income + balance, family = "binomial", 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4739  -0.1494  -0.0597  -0.0223   3.7009  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(>|z|)    
## (Intercept) -1.145e+01  4.447e-01 -25.734  < 2e-16 ***
## income       2.192e-05  5.106e-06   4.294 1.76e-05 ***
## balance      5.585e-03  2.321e-04  24.069  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2737.5  on 8999  degrees of freedom
## Residual deviance: 1482.2  on 8997  degrees of freedom
## AIC: 1488.2
## 
## Number of Fisher Scoring iterations: 8
## 
##      y.val.class
##        No Yes
##   No  982   1
##   Yes  13   4
##  오분류율    민감도    특이도    정밀도    재현율 
## 0.0140000 0.2352941 0.9989827 0.8000000 0.2352941
## Setting levels: control = No, case = Yes
## Setting direction: controls < cases
## Area under the curve: 0.9552
```

```r
cv(0.8) # tr:val = 8:2
## 
## Call:
## glm(formula = default ~ income + balance, family = "binomial", 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4297  -0.1372  -0.0541  -0.0195   3.7663  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(>|z|)    
## (Intercept) -1.153e+01  4.965e-01 -23.222  < 2e-16 ***
## income       1.543e-05  5.701e-06   2.707  0.00679 ** 
## balance      5.709e-03  2.612e-04  21.859  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2266.0  on 7999  degrees of freedom
## Residual deviance: 1212.8  on 7997  degrees of freedom
## AIC: 1218.8
## 
## Number of Fisher Scoring iterations: 8
## 
##      y.val.class
##         No  Yes
##   No  1920    3
##   Yes   54   23
##  오분류율    민감도    특이도    정밀도    재현율 
## 0.0285000 0.2987013 0.9984399 0.8846154 0.2987013
## Setting levels: control = No, case = Yes
## Setting direction: controls < cases
## Area under the curve: 0.9372
```

```r
cv(0.3) # tr:val = 3:7
## 
## Call:
## glm(formula = default ~ income + balance, family = "binomial", 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2374  -0.1320  -0.0497  -0.0175   3.2084  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(>|z|)    
## (Intercept) -1.254e+01  8.811e-01 -14.238  < 2e-16 ***
## income       3.290e-05  9.744e-06   3.377 0.000733 ***
## balance      5.975e-03  4.513e-04  13.239  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 849.76  on 2999  degrees of freedom
## Residual deviance: 443.54  on 2997  degrees of freedom
## AIC: 449.54
## 
## Number of Fisher Scoring iterations: 8
## 
##      y.val.class
##         No  Yes
##   No  6734   29
##   Yes  163   74
##   오분류율     민감도     특이도     정밀도     재현율 
## 0.02742857 0.31223629 0.99571196 0.71844660 0.31223629
## Setting levels: control = No, case = Yes
## Setting direction: controls < cases
## Area under the curve: 0.9473
```
<img src="/images/2020-09/islr13-3.png"  width="700" height="370">

- 훈련세트와 검증세트를 나누는 비율을 다르게 해본 결과,
- 9:1의 극단적인 비율에서는 오분류율이 작게 나오는 것을 볼 수 있다.
- 8:2와 3:7의 비율에서는 민감도, 특이도, 정밀도, 재현율이 비슷하다.

## 5.5 (d) 더미변수 student 추가, test error 추정 using validation set approach
```r
df<-Default
#df$student <- ifelse(df$student=="Yes", 2, 1)
fit.glm <- glm(default ~ income + balance + student, data=df, family='binomial')
summary(fit.glm)
## 
## Call:
## glm(formula = default ~ income + balance + student, family = "binomial", 
##     data = df)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4691  -0.1418  -0.0557  -0.0203   3.7383  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(>|z|)    
## (Intercept) -1.087e+01  4.923e-01 -22.080  < 2e-16 ***
## income       3.033e-06  8.203e-06   0.370  0.71152    
## balance      5.737e-03  2.319e-04  24.738  < 2e-16 ***
## studentYes  -6.468e-01  2.363e-01  -2.738  0.00619 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2920.6  on 9999  degrees of freedom
## Residual deviance: 1571.5  on 9996  degrees of freedom
## AIC: 1579.5
## 
## Number of Fisher Scoring iterations: 8
#income, balance가 설명변수였을 때의 모델에서는 두 변수의 회귀계수 모두 유의수준 0.05하에서 유의하다 판단되지만, student 변수를 추가한 뒤 모델을 적합시켰을 때에는 income 변수의 회귀계수가 유의수준 0.05하에서 유의하지 않다 판단된다.

set.seed(1)
n = nrow(df)
ind.shuff <- sample(1:n, size = n)
n.tr <- floor(n * 0.7)
n.val <- n - n.tr

train <- df[ind.shuff[1:n.tr],]
val <- df[ind.shuff[(n.tr+1):n],]

#predict()사용 시
prob <- predict(fit.glm, newdata=val, type="response")
class <- ifelse(prob > 0.5, "Yes", "No")
mean(class!=val$default) # 0.02733333
## [1] 0.02733333
############################
# 과제 외적으로 행렬 계산을 이용한 풀이를 해보았습니다. 하지만, 팩터이기 때문에 계산이 제대로 되지 않아, 우선 predict를 사용하였고, 이 부분은 주석 처리합니다. 이 부분이 궁금하여 추후에 질문하려고 남겨두었습니다.

# fit.glm.train <- glm(default ~ income + balance + student, data = train, family = "binomial")
# summary(fit.glm.train)
# 
# x.val = as.matrix(val[c("income", "balance","student")])
# 
# y.val.hat <- cbind(1, x.val) %*% coef(fit.glm.train)
# fit.prob <- exp(y.val.hat) / (1 + exp(y.val.hat))
# 
# y.val.class <- ifelse(fit.prob > 0.5, "Yes", "No")
# 
# mat <- table(val$default, y.val.class)
# print(mat) # 정오행렬
# 
# result <- (mat[2]+mat[3]) / sum(mat)
# result # 0.02766667
############################


# student 지시변수를 추가한 모델을 이용하여 구한 검증오류는 0.02733333로 앞의 income, balance 변수만을 이용하여 적합한 모델의 검증오류(0.0254000)보다는 조금 크게 나왔다.
# student 지시변수가 유의하다 판단 될 만큼의 큰 차이라고는 없다고 판단된다.  
```