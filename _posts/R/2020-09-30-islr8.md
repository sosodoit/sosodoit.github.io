---
layout: page
title: "[ISLR] 데이터마이닝 솔루션 (8)"
description: ch3
headline:
categories: data-science
tags: [data-science]
comments: true
published: true 
---

## 14-(a)
```r
set.seed(1)
x1 = runif(100)
x2 = 0.5 * x1 + rnorm(100)/10
y = 2 + 2 * x1 + 0.3 * x2 + rnorm(100)

###Q) Write out the form of the linear model
#sol)
#y = beta0 + beta1*x1 + beta2*x2 + ε => 2개의 설명변수(x1, x2)가 있는 linear model
#위 문제에서 linear model의 form은 "y = 2 + 2*x1 + 0.3*x2 + ε , ε ~ N(0,1)" 이다.

###Q) What are the regression coefficients?
#sol) 
#회귀계수는 beta0=2, beta1=2, beta2=0.3 임을 알 수 있다.
```

## 14-(b)
```r
cor(x1, x2)
## [1] 0.8351212

cor.test(x1, x2)
## 
##  Pearson's product-moment correlation
## 
## data:  x1 and x2
## t = 15.03, df = 98, p-value < 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.7640291 0.8861771
## sample estimates:
##       cor 
## 0.8351212

par(mfrow = c(1,1))
plot(x1, x2, col="red", pch =20, main="Scatter Plot of x1 and x2")
```
<img src="/images/2020-09/314b.png"  width="700" height="370">

- Q) x1과 x2의 상관 관계
- sol) 
- x1과 x2의 pearson 상관계수 = 0.8351212 로서 상당히 높은 양의 상관관계를 가지는 것으로 나왔다. 따라서 x1과 x2 사이에 공선성을 의심할 수 있어 보인다. (상관계수는 1에 가까울수록 강한 양의 선형관계를 가지므로)

## 14-(c)
```r
fit <- lm(y ~ x1 + x2)
summary(fit)
## 
## Call:
## lm(formula = y ~ x1 + x2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.8311 -0.7273 -0.0537  0.6338  2.3359 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   2.1305     0.2319   9.188 7.61e-15 ***
## x1            1.4396     0.7212   1.996   0.0487 *  
## x2            1.0097     1.1337   0.891   0.3754    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.056 on 97 degrees of freedom
## Multiple R-squared:  0.2088, Adjusted R-squared:  0.1925 
## F-statistic:  12.8 on 2 and 97 DF,  p-value: 1.164e-05
```
- Q) fit a least squares regression to predict y using x1, x2
- sol) 
- least squares regression 적합 결과,

- H0 : 회귀모형은 타당하지 않다 vs  H1 : 회귀모형은 타당하다
유의확률이 1.164e-05이므로 유의수준 0.05에서 귀무가설을 기각할 수 있다.
따라서, 유의수준 0.05하에서 위의 회귀모형은 타당하다고 판단된다.
Multiple-R-squared ==> 회귀모형의 설명력(독립변수의 설명력)은 약 20.88% 정도이다.

- β0 hat = 2.1305, β1 hat = 1.4396, β2 hat = 1.0097
β0 = 2, β1 = 2, β2 = 0.3 true 값과 비교해보면
β0와 β0 hat은 비슷하다.
β1와 β1 hat은 2-1.4396 = 0.5 이상이 차이가 난다.
β2는 β2 hat은 0.3-1.0097 = 0.7 이상의 차이가 난다.

- H0 : β1 = 0 vs  H1 : β1은 0이 아니다.
x1의 p-value가 0.0487이므로 유의수준 0.05하에서 β1에 대한 귀무가설을 기각할 수 있다. 따라서 유의수준 0.05하에서 β1은 0이 아니라고 할 수 있다.

- H0 : β2 = 0 vs  H1 : β2은 0이 아니다.
x2의 p-value가 0.3754이므로 유의수준 0.05하에서 β2에 대한 귀무가설을 기각할 수 없다. 따라서 유의수준 0.05하에서 β2는 0이라고 할 수 있다.

## 14-(d)
```r
fit2 <- lm(y ~ x1)
summary(fit2)
## 
## Call:
## lm(formula = y ~ x1)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.89495 -0.66874 -0.07785  0.59221  2.45560 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   2.1124     0.2307   9.155 8.27e-15 ***
## x1            1.9759     0.3963   4.986 2.66e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.055 on 98 degrees of freedom
## Multiple R-squared:  0.2024, Adjusted R-squared:  0.1942 
## F-statistic: 24.86 on 1 and 98 DF,  p-value: 2.661e-06
```
- Q) fit a least squares regression to predict y using only x1
- sol) 
- least squares regression 적합 결과,
- H0 : 회귀모형은 타당하지 않다 vs  H1 : 회귀모형은 타당하다
유의확률이 2.661e-06이므로 유의수준 0.05에서 귀무가설을 기각할 수 있다.
따라서, 유의수준 0.05하에서 회귀모형은 타당하다고 판단된다.
Multiple-R-squared ==> 회귀모형의 설명력(독립변수의 설명력)은 약 20.24% 정도이다.

- β0 hat = 2.1124, β1 hat = 1.9759
β0 = 2, β1 = 2 true 값과 비교해보면
β0와 β0 hat은 비슷하다.
β1와 β1 hat은 비슷하다. x1의 coefficient는 (c)에서와 다른 결과를 보였다.

- H0 : β1 = 0 vs  H1 : β1은 0이 아니다.
x1 p-value가 2.66e-06으로 유의수준 0.05하에서 β1 = 0이라는 귀무가설을 기각할 수 있다. 따라서 유의수준 0.05하에서 β1은 0이 아니라고 할 수 있다.

## 14-(e)
```r
fit3 <- lm(y ~ x2)
summary(fit3)
## 
## Call:
## lm(formula = y ~ x2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.62687 -0.75156 -0.03598  0.72383  2.44890 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   2.3899     0.1949   12.26  < 2e-16 ***
## x2            2.8996     0.6330    4.58 1.37e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.072 on 98 degrees of freedom
## Multiple R-squared:  0.1763, Adjusted R-squared:  0.1679 
## F-statistic: 20.98 on 1 and 98 DF,  p-value: 1.366e-05
```
- Q) fit a least squares regression to predict y using only x2
- sol) 
- least squares regression 적합 결과,
- H0 : 회귀모형은 타당하지 않다 vs  H1 : 회귀모형은 타당하다
유의확률이 1.366e-05이므로 유의수준 0.05에서 회귀모형은 통계적으로 타당하다.
Multiple-R-squared ==> 회귀모형의 설명력(독립변수의 설명력)은 약 17.63% 정도이다.

- β0 hat = 2.3899, β1 hat = 2.8996

β0 = 2, β1 = 0.3 true 값과 비교해보면
β0와 β0 hat은 비슷하다.
β1와 β1 hat은 상당히 큰 차이를 보인다. x2의 coefficient는 (c)에서와 다른 결과를 보였다.

- H0 : β1 = 0 vs  H1 : β1은 0이 아니다.
x2의 p-value가 1.37e-05로 유의수준 0.05하에서 β1 = 0이라는 귀무가설을 기각할 수 있다. 따라서 유의수준 0.05하에서 β1은 0이 아니라고 할 수 있다.

## 14-(f)
```r
fit$coefficients
## (Intercept)          x1          x2 
##    2.130500    1.439555    1.009674
fit2$coefficients
## (Intercept)          x1 
##    2.112394    1.975929
fit3$coefficients
## (Intercept)          x2 
##    2.389949    2.899585
```
- Q) (c)-(e) contradict each other? explain your answer.
- sol) 
- fit2와 fit3에서 x1, x2 두 변수를 모두 이용하여 최소제곱회귀를 적합 했을 때와는 다른 결과가 x2변수에서 보였다. 

x1 변수만 이용하여 적합한 fit2에서는 x1의 p-value가 매우 작게 나와 x1이 유의하다 판단했다.
x2 변수만 이용하여 적합한 fit3에서는 유의수준 0.05하에서 유의하지 않던 x2가 유의하다는 결과가 나왔다.

즉, x2의 회귀계수 유의성 검정에서 모순된 결과를 얻었다는 것을 확인할 수 있다.

이는 독립변수 x1과 x2 사이에 연관성이 있기 때문에 이러한 결과가 나타는 것으로 보인다. 앞서 (b)에서 확인한 x1과 x2의상관계수 = 0.8351212 로서 상당히 높은 양의 상관관계를 가지는 것을 통해 알 수 있다.

어떤 한 설명번수를 모형에 제거(or추가)하는 것이 추정된 회귀계수의 크기(or 부호)에 큰 변화를 준 것으로 보아 이들 사이의 공선성을 의심할 수 있다. 

## 14-(g)
```r
x1 = c(x1, 0.1)
x2 = c(x2, 0.8)
y = c(y, 6)

fit_new <- lm(y ~ x1 + x2)
summary(fit_new)
## 
## Call:
## lm(formula = y ~ x1 + x2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.73348 -0.69318 -0.05263  0.66385  2.30619 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   2.2267     0.2314   9.624 7.91e-16 ***
## x1            0.5394     0.5922   0.911  0.36458    
## x2            2.5146     0.8977   2.801  0.00614 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.075 on 98 degrees of freedom
## Multiple R-squared:  0.2188, Adjusted R-squared:  0.2029 
## F-statistic: 13.72 on 2 and 98 DF,  p-value: 5.564e-06
#least squares regression 적합 결과,
#β0 hat = 2.2267, β1 hat = 0.5394, β2 hat = 2.5146

#H0 : β1 = 0 vs  H1 : β1은 0이 아니다.
#x1의 p-value가 0.36458로 유의수준 0.05하에서 귀무가설을 기각할 수가 없다. 따라서 유의수준 0.05하에서 β1은 0이라고 할 수 있다.

#H0 : β2 = 0 vs  H1 : β2은 0이 아니다.
#x2 p-value가 0.00614로 유의수준 0.05하에서 귀무가설을 기각할 수 있다. 따라서 유의수준 0.05하에서 β2은 0이 아니라고 할 수 있다.

#fit_new에서 위의 fit결과와는 다르게 x2의 변수가 유의하다고 나왔다.

fit2_new <- lm(y ~ x1)
summary(fit2_new)
## 
## Call:
## lm(formula = y ~ x1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.8897 -0.6556 -0.0909  0.5682  3.5665 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   2.2569     0.2390   9.445 1.78e-15 ***
## x1            1.7657     0.4124   4.282 4.29e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.111 on 99 degrees of freedom
## Multiple R-squared:  0.1562, Adjusted R-squared:  0.1477 
## F-statistic: 18.33 on 1 and 99 DF,  p-value: 4.295e-05
#least squares regression 적합 결과,
#β0 hat = 2.2569, β1 hat = 1.7657

#H0 : β1 = 0 vs  H1 : β1은 0이 아니다.
#x1의 p-value가 4.29e-05로 유의수준 0.05하에서 귀무가설 기각할 수 있다. 따라서 유의수준 0.05하에서 β1은 0이 아니라고 할 수 있다.

#x1, x2 두 변수를 모두 이용하여 적합한 fit_new에서는 x1의 회귀계수가 유의수준 0.05하에서 유의하지 않다고 나왔지만,
#x1 변수만을 이용하여 적합한 fit2_new에서는 x1 변수의 회귀계수가 유의수준 0.05하에서 유의하다는 결과가 나왔다.

fit3_new <- lm(y ~ x2)
summary(fit3_new)
## 
## Call:
## lm(formula = y ~ x2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.64729 -0.71021 -0.06899  0.72699  2.38074 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   2.3451     0.1912  12.264  < 2e-16 ***
## x2            3.1190     0.6040   5.164 1.25e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.074 on 99 degrees of freedom
## Multiple R-squared:  0.2122, Adjusted R-squared:  0.2042 
## F-statistic: 26.66 on 1 and 99 DF,  p-value: 1.253e-06
#least squares regression 적합 결과,
#β0 hat = 2.3451, β1 hat = 3.1190

#H0 : β1 = 0 vs  H1 : β1은 0이 아니다.
#x2의 p-value가 1.25e-06로 유의수준 0.05하에서 귀무가설 기각할 수 있다. 따라서 유의수준 0.05하에서 β1은 0이 아니라고 할 수 있다.

#x1, x2 두 변수를 모두 이용하여 적합한 fit_new에서는 x2의 회귀계수가 유의수준 0.05하에서 유의하다고 나왔고, x2 변수만을 이용하여 적합한 fit3_new에서 또한 x2 변수의 회귀계수가 유의수준 0.05하에서 유의하다는 결과가 나왔다.


###Q) In each model, is this obs an outlier? A high-leverage point? Both?
par(mfrow=c(1,3))

#x1~x2 
plot(fit_new, which=c(1,5))
#Residuals vs Fitted를 보면 완벽한 수평선 모형은 아니더라도 잔차의 평균이 0에 가까운 선을 보인다. 다만 point 21, 55, 82가 outlier로 보인다.
#Residuals vs Leverage를 보면 point 101가 high leverage point&outlier이다.
plot(x1,x2, main="x2~x1") #(변수들간의 관계확인)
grid()
points(0.1, 0.8, col = "red", type = "p", pch = 15)
abline(v = mean(x1), h = mean(x2), col = c("green", "blue"))
```
<img src="/images/2020-09/314g.png"  width="700" height="370">

```r
#y~x1
plot(fit2_new, which=c(1,5))
##no high liverage point
#Residuals vs Fitted를 보면 완벽한 수평선 모형은 아니더라도 잔차의 평균이 0에 가까운 선을 보인다. 다만 point 101, 55, 82가 outlier로 보이고 Residuals vs Leverage를 보면 point 101,21,55가 high leverage point로 보이지만 대게 point들이 왼쪽에 모여있지 않고 퍼져있으므로 leverage 인지 판단하기 어렵다. 그래서 다음과 같이 influence()함수를 이용하여 확인해보았다.
plot(x1, y, main="y~x1")
grid()
abline(coef(fit2_new))
points(0.1, 6, col = "red", type = "p", pch = 15)
abline(v = mean(x1), h = mean(y), col = c("green", "blue"))
```
<img src="/images/2020-09/314g2.png"  width="700" height="370">

```r
#outlier, high leverage 기준
#high leverage : diagonal elements of hat matrix ==> hii > 3p'/n 
#good leverage points : leverage가 높지만 residual이 작은 데이터
#bad leverage points = outlier : leverage도 높지만 residual도 큰 데이터
par(mfrow=c(1,2))
influ2 <- influence(fit2_new)
idx2 <- influ2$hat > 3*sum(influ2$hat)/101
plot(influ2$hat, main="Leverage")
lines(influ2$hat, type="h")
plot(y~x1, main="y~x1")
abline(fit2_new, col=rgb(0.1,0.6,0.1,0.5), lwd=3)
grid()
points(x1[idx2], y[idx2], col=rgb(0.8,0.2,0.2,0.5), pch=16, cex=5)
```
<img src="/images/2020-09/314g3.png"  width="700" height="370">

```r
#위 결과, high leverage에 대한 point가 그림에 나오지 않는다. 따라서 이 기준(hii > 3p'/n)에서는 high leverage가 없다고 판단된다. 

#y~x2
par(mfrow=c(1,3))
plot(fit3_new, which=c(1,5))
##high leverage point
#Residuals vs Fitted를 보면 완벽한 수평선 모형은 아니더라도 잔차의 평균이 0에 가까운 선을 보인다. 다만 point 21,55,82가 outlier로 보인다.
#Residuals vs Leverage를 보면 point 22와 55는 low leverage면서 outlier, 101은 high leverage point로 보인다.
plot(x2, y, main="y~x2")
grid()
abline(coef(fit3_new))
points(0.8, 6, col = "red", type = "p", pch = 15)
abline(v = mean(x2), h = mean(y), col = c("green", "blue"))
```
<img src="/images/2020-09/314g4.png"  width="700" height="370">

```r
par(mfrow=c(1,2))
influ3 <- influence(fit3_new)
plot(influ3$hat, main="Leverage")
idx3 <- influ3$hat > 3*sum(influ3$hat)/101
lines(influ3$hat, type="h")
plot(y~x2, main="y~x2")
abline(fit3_new, col=rgb(0.1,0.6,0.1,0.5), lwd=3)
grid()
points(x2[idx3], y[idx3], col=rgb(0.8,0.2,0.2,0.5), pch=16, cex=5)
```
<img src="/images/2020-09/314g5.png"  width="700" height="370">

- y~x1과 다르게 우리가 plot(x2,y)에 그린 것과 influence를 이용해 확인한 high leverage가 그림에 나오는 것을 확인할 수 있다.
- 각 모델에서 전반적으로 101번째 관측치가 high liverage & outlier로 보인다.