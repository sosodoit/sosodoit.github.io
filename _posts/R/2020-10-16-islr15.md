---
layout: page
title: "[ISLR] 데이터마이닝 솔루션 (15)"
description:
headline: 
categories: data-science
tags: [data-science]
comments: true
published: true
---

회귀 모형평가 5-fold CV로 변경하고 푸세요.

## 5.8 (a) cross-validation
```r
set.seed(1)
x = rnorm(100)
y = x - 2 * x^2 + rnorm(100)

# n은 관측치의 수이므로, 100이다.
# p는 변수의 수이므로, 2이다.
```

## 5.8 (b)
```r
plot(x, y, col='skyblue')
```
<img src="/images/2020-09/islr15-1.png"  width="700" height="370">

- x, y의 관계가 위로 볼록인 형태 즉, 이차함수 형태(포물선 그림)처럼 나타난다. 
- points가 볼록 튀어 나온 지점에 많이 몰려 있어 보인다.
- 따라서 적합시에, 선형회귀모형을 적용하는 것보다 포물선 형태의 모형이 더 적절할 것 같다.

## 5.8 (c) compute the 5-CV errors
```r
# 각 모형의 5-CV errors를 계산하는 함수로
K = 5; n = 100

cv.mse.f <- function(seednum) {
  
  set.seed(seednum)
  data <- data.frame(x, y)
  
  mse.res <- numeric(4) # 각 모형의 MSE 저장
  mse.glm = numeric(K)  # 한 모형에서의 5번 cv한 MSE 평균을 저장
  
  for (p in 1:4) {
    ind.shf = sample(1:n, size = n)
    
    for (k in 1:K) {
      ind.val = ind.shf[(floor(n/K)*(k-1)+1):(floor(n/K)*k)]
      ind.tr = setdiff(1:n, ind.val)
      df.tr = data[ind.tr,]
      df.val = data[ind.val,]
      
      x.tr = df.tr$x
      y.tr = df.tr$y
      x.val = df.val$x
      y.val = df.val$y
      
      xmat = matrix(0, nrow=nrow(df.tr), ncol=p)
      xmat.val = matrix(0, nrow=nrow(df.val), ncol=p)
      
      for (j in 1:p) { 
        xmat[ ,j] = x.tr^j
        xmat.val[,j] = x.val^j
      }
      
      glm.fit = glm(y.tr ~ xmat)
      
      y.val.hat = as.matrix(cbind(1, xmat.val)) %*% coef(glm.fit)
      mse.glm[k] = mean((y.val - y.val.hat)^2)
    }
    mse.res[p] <- mean(mse.glm)
  }
  mse.res
}

cv.mse.f(1) 
# seednum=1, 순서대로 i ~ iv 번모형의 5-CV errors
## [1] 8.1796572 0.9655600 0.9884602 1.0003704
```

## 5.8 (d) using another random seed

```r
cv.mse.seed <- matrix(ncol = 4, nrow = 50) # seed값을 다르게 해서 MSE 저장 
rs <- sample(1:1000, size=50) # seed에 넣을 값을 랜덤하게 50개 정도 뽑아서 해봤습니다!
for (i in 1:50) {
  cv.mse.seed[i,] <- cv.mse.f(rs[i])
}
apply(cv.mse.seed, 2, mean)
## [1] 7.2975797 0.9478024 0.9768914 1.0337484
# (c)번의 결과와 비슷해 보인다. 평균적으로 2차의 모형일 때, mse가 제일 작은 것을 볼 수 있다.
# (b)번에서 우리가 예상했던 대로 포물선 형태인 2차가 가장 잘 적합하게 된 것 같다. 
```

## 5.8 (e) the smallest 5-cv error

<img src="/images/2020-09/islr15-2.png">

- (c)번의 결과를 보면, 가장 작은 5-cv error를 갖는 모형은 두 번째 모형이었다.
- 우리가 예상했던대로 두 번째 모형인 2차함수모형이 결과로 나온 것을 확인하였다.
- (a)번에서도 우리가 true model이 2차 함수 였기 때문에 이러한 결과가 나오는 것이 타당해 보인다.

## 5.8 (f) fitting each of the models in (c) using least squares. 전체데이터셋으로 적합한 결과와 cv를 사용한 결과에 대한 비교로 이해

```r
set.seed(1)
data<-as.data.frame(cbind(x,y))

model1<-glm(y~x,data=data)
summary(model1)$coef
##              Estimate Std. Error   t value     Pr(>|t|)
## (Intercept) -1.625427  0.2619366 -6.205420 1.309300e-08
## x            0.692497  0.2909418  2.380191 1.923846e-02
#--------------------------1번 모형--------------------------
# x의 회귀계수 p-value가 통상적인 유의수준 0.05보다 작으므로 
# 유의수준 0.05하에서 x의 회귀계수가 0이 아니라고(유의하다고) 할 수 있다.


model2<-glm(y~x+I(x^2),data=data)
summary(model2)$coef
##                Estimate Std. Error    t value     Pr(>|t|)
## (Intercept)  0.05671501  0.1176555   0.482043 6.308613e-01
## x            1.01716087  0.1079827   9.419666 2.403287e-15
## I(x^2)      -2.11892120  0.0847657 -24.997388 4.584330e-44
#--------------------------2번 모형--------------------------
# x의 회귀계수, x^2의 회귀계수들의  p-value가 모두 통상적인 유의수준 0.05보다 작으므로 
# 유의수준 0.05하에서 x의 회귀계수, x^2의 회귀계수가 0이 아니라고(유의하다고) 할 수 있다.

model3<-glm(y~x+I(x^2)+I(x^3),data=data)
summary(model3)$coef
##                Estimate Std. Error     t value     Pr(>|t|)
## (Intercept)  0.06150718 0.11950374   0.5146883 6.079538e-01
## x            0.97528027 0.18728149   5.2075636 1.089350e-06
## I(x^2)      -2.12379099 0.08700251 -24.4106856 5.873444e-43
## I(x^3)       0.01763858 0.06429037   0.2743580 7.843990e-01
#--------------------------3번 모형--------------------------
# x의 회귀계수, x^2의 회귀계수들의  p-value가 모두 통상적인 유의수준 0.05보다 작으므로 
# 유의수준 0.05하에서 x의 회귀계수, x^2의 회귀계수가  0이 아니라고(유의하다고) 할 수 있다.
# 반면, x^3의 회귀계수의 p-value는 유의수준 0.05보다 크므로 
# 유의수준 0.05하에서 x^3의 회귀계수는 0이라 할 수 있다.즉, 유의하지 않다.

model4<-glm(y~x+I(x^2)+I(x^3)+I(x^4),data=data)
summary(model4)$coef
##                 Estimate Std. Error     t value     Pr(>|t|)
## (Intercept)  0.156702953 0.13946192   1.1236253 2.640034e-01
## x            1.030825643 0.19133655   5.3874999 5.174326e-07
## I(x^2)      -2.409898183 0.23485506 -10.2612148 4.575229e-17
## I(x^3)      -0.009132904 0.06722881  -0.1358481 8.922288e-01
## I(x^4)       0.069785421 0.05324006   1.3107691 1.930956e-01
#--------------------------4번 모형--------------------------
# x의 회귀계수, x^2의 회귀계수들의  p-value가 모두 통상적인 유의수준 0.05보다 작으므로 
# 유의수준 0.05하에서 x의 회귀계수, x^2의 회귀계수가  0이 아니라고(유의하다고) 할 수 있다.
# 반면, x^3의 회귀계수, x^4의 회귀계수들의 p-value는 유의수준 0.05보다 크므로 
# 유의수준 0.05하에서 x^3의 회귀계수, x^4의 회귀계수는 0이라 할 수 있다.즉, 유의하지 않다.

# 결과적으로 유의수준 5%에서, 4개의 모형 모두 x와, x^2의 회귀계수만이 통계적으로 유의한 결과를 보였다.
# 따라서 두 번째 모형인, 2차 함수 모형이 주어진 데이터셋에 대하여 가장 적합한 모형이라고 판단된다.
# 이는 앞서 cross-validation 결과와도 동일함을 확인하였다.
```