---
layout: page
title: "[ISLR] 데이터마이닝 솔루션 (18)"
description: 
headline: 
categories: data-science
tags: [data-science]
comments: true
published: true
---

```r
# library load
library(ISLR)
library(tree)
library(glmnet)
library(dbplyr)
library(ggplot2)
```

# 8.9 교차검증(cross-validation)은 5-fold로, 하드코딩 구현

## [a] train set - 800 obs & test set - remaining obs
```r
data(OJ)
set.seed(1)
ind<-sample(1:nrow(OJ),size=nrow(OJ))
ind.tr<-ind[1:800]

train <- OJ[ind.tr,]
test <- OJ[-ind.tr,]
```

## [b] train - fit a tree, y=Purchase
```r
fit.tree = tree(Purchase~., data=train)
summary(fit.tree)
## 
## Classification tree:
## tree(formula = Purchase ~ ., data = train)
## Variables actually used in tree construction:
## [1] "LoyalCH"       "PriceDiff"     "SpecialCH"     "ListPriceDiff"
## [5] "PctDiscMM"    
## Number of terminal nodes:  9 
## Residual mean deviance:  0.7432 = 587.8 / 791 
## Misclassification error rate: 0.1588 = 127 / 800
```
> 재귀적 이진분할에 의해 처음 쓰인 변수는 LoyalCH이다. 반응변수를 제외 총 17개의 변수 중 5개의 변수("LoyalCH","PriceDiff","SpecialCH","ListPriceDiff","PctDiscMM")들이 의사결정나무 모형에 사용되었다.
  >What is the training error rate? 0.1588(15.88%)
  >How many terminal nodes does the tree have? 9 

## [c]
```r
fit.tree
## node), split, n, deviance, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 800 1073.00 CH ( 0.60625 0.39375 )  
##    2) LoyalCH < 0.5036 365  441.60 MM ( 0.29315 0.70685 )  
##      4) LoyalCH < 0.280875 177  140.50 MM ( 0.13559 0.86441 )  
##        8) LoyalCH < 0.0356415 59   10.14 MM ( 0.01695 0.98305 ) *
##        9) LoyalCH > 0.0356415 118  116.40 MM ( 0.19492 0.80508 ) *
##      5) LoyalCH > 0.280875 188  258.00 MM ( 0.44149 0.55851 )  
##       10) PriceDiff < 0.05 79   84.79 MM ( 0.22785 0.77215 )  
##         20) SpecialCH < 0.5 64   51.98 MM ( 0.14062 0.85938 ) *
##         21) SpecialCH > 0.5 15   20.19 CH ( 0.60000 0.40000 ) *
##       11) PriceDiff > 0.05 109  147.00 CH ( 0.59633 0.40367 ) *
##    3) LoyalCH > 0.5036 435  337.90 CH ( 0.86897 0.13103 )  
##      6) LoyalCH < 0.764572 174  201.00 CH ( 0.73563 0.26437 )  
##       12) ListPriceDiff < 0.235 72   99.81 MM ( 0.50000 0.50000 )  
##         24) PctDiscMM < 0.196197 55   73.14 CH ( 0.61818 0.38182 ) *
##         25) PctDiscMM > 0.196197 17   12.32 MM ( 0.11765 0.88235 ) *
##       13) ListPriceDiff > 0.235 102   65.43 CH ( 0.90196 0.09804 ) *
##      7) LoyalCH > 0.764572 261   91.20 CH ( 0.95785 0.04215 ) *
```

>8번째 terminal node를 예로 들어보자. LoyalCH가 0.5036을 기준으로 처음 분기가 결정된다. 그 다음으로 LoyalCH가 0.280875보다 작은 기준으로 분기가 다시 나뉘고, 그 후에 LoyalCH < 0.0356415에 속하는 지점이 8번째 terminal node에 해당한다. 여기서의 관측치는 59개, deviance는 10.14, Purchase가 MM으로 예측된다.CH의 개수와 MM개수의 비율은 0.01695:0.98305이다.

## [d]
```r
plot(fit.tree)
text(fit.tree, pretty = 0)
```
<img src="/images/2020-09/islr18-1.png">

> root node는 LoyalCH< 0.5036의 기준으로 나뉘고 Purchase의 LoyalCH가 0.280875보다 작은 값은 "MM"으로 분류되며, 큰 값은 PriceDiff를 기준으로 분류된다. 이러한 과정을 통해(다시말해, LoyalCH, PriceDiff, ListPriceDiff, PctDiscMM, specialCH 이 변수들에 의해) terminal node가 정해진다. terminal node에 의해 나뉜 영역은 총 9개이다. tree모형을 적합시키기 위해 제일 많이 사용된 설명변수는 LoyalCH라는 것을 알 수 있었다. 따라서 가장 중요한 지표는 LoyalCH이다. 

## [e] Predict the response on the test data
```r
set.seed(1)
yhat = predict(fit.tree, test, type = "class")
tbl = table(yhat, test$Purchase); tbl
##     
## yhat  CH  MM
##   CH 160  38
##   MM   8  64
test.error <- (tbl[1,2]+tbl[2,1])/nrow(test)
cat("\ntest error = ", test.error,"%")
## 
## test error =  0.1703704 %
## What is the test error rate? 0.1703704(17.03%)
```

## [f] determine the optimal tree size.
```r
K = 5
store.e = numeric(K) #오분류율 저장
tree.size = numeric(K)

cv5.tree <- function() {
  for (k in 1:K) {
    set.seed(k*1004)
    n = nrow(train)
    ind.shf = sample(1:n, size = n)
    
    ind.val = ind.shf[(floor(n/K)*(k-1)+1):(floor(n/K)*k)]
    ind.tr = setdiff(1:n, ind.val)
    df.tr = train[ind.tr,]
    df.val = train[ind.val,]
    
    cv.fit = tree(Purchase~., data = df.tr)
    cv.pred = predict(cv.fit, df.val, type = "class")
    
    tbl <- table(cv.pred, df.val$Purchase)
    
    store.e[k] = (tbl[2] + tbl[3]) / nrow(df.val)
    tree.size[k] = summary(cv.fit)$size
}
  store = cbind(tree.size, store.e)
  return(store)
}
store = cv5.tree()

minerror <- store[which.min(store[,2]),][1]
```

## [g] x : tree size & y : 5-fold 분류오류율
```r
store = store[order(store[,1], store[,2]),]
plot(store[,1], store[,2], type = "b")
```
<img src="/images/2020-09/islr18-2.png">

## [h] 
```r
cat("\n최적의 tree size = ", minerror)
## 
## 최적의 tree size =  10
```

## [i]
```r
[i] pruned tree
fit <- tree(Purchase~., data = train)

fit.total1 <- prune.tree(fit, best=10) #best is bigger than tree size
## Warning in prune.tree(fit, best = 10): best is bigger than tree size
summary(fit.total1)
## 
## Classification tree:
## tree(formula = Purchase ~ ., data = train)
## Variables actually used in tree construction:
## [1] "LoyalCH"       "PriceDiff"     "SpecialCH"     "ListPriceDiff"
## [5] "PctDiscMM"    
## Number of terminal nodes:  9 
## Residual mean deviance:  0.7432 = 587.8 / 791 
## Misclassification error rate: 0.1588 = 127 / 800
fit.total2 <- prune.tree(fit, best=5)
summary(fit.total2)
## 
## Classification tree:
## snip.tree(tree = fit, nodes = c(4L, 12L, 5L))
## Variables actually used in tree construction:
## [1] "LoyalCH"       "ListPriceDiff"
## Number of terminal nodes:  5 
## Residual mean deviance:  0.8239 = 655 / 795 
## Misclassification error rate: 0.205 = 164 / 800
```
>교차검증오류를 이용한 최적의 model이 pruned tree를 선택하지 않았으므로 terminal node가 5개인 tree 강제로 만듦.

## [j] Compare the training error rates between the pruned and unpruned trees. Which is higher?
```r
summary(fit.tree)
## 
## Classification tree:
## tree(formula = Purchase ~ ., data = train)
## Variables actually used in tree construction:
## [1] "LoyalCH"       "PriceDiff"     "SpecialCH"     "ListPriceDiff"
## [5] "PctDiscMM"    
## Number of terminal nodes:  9 
## Residual mean deviance:  0.7432 = 587.8 / 791 
## Misclassification error rate: 0.1588 = 127 / 800
summary(fit.total2)
## 
## Classification tree:
## snip.tree(tree = fit, nodes = c(4L, 12L, 5L))
## Variables actually used in tree construction:
## [1] "LoyalCH"       "ListPriceDiff"
## Number of terminal nodes:  5 
## Residual mean deviance:  0.8239 = 655 / 795 
## Misclassification error rate: 0.205 = 164 / 800
```

* unpruned tree의 training error는 0.1588((b)에서 적합)이고 pruned tree의 training error는 0.205이다. pruned tree의 훈련오류율이 더 높다.

## [k] Compare the test error rates between the pruned and unpruned trees. Which is higher?

```r
set.seed(1)
yhat.test = predict(fit.total2, test, type = "class")
tbl <- table(yhat.test, test$Purchase)
test.error.p = (tbl[2]+tbl[3])/nrow(test)

test.error.p # pruned tree test error
## [1] 0.1962963
test.error # unpruned tree test error
## [1] 0.1703704
```

* unpruned tree : test error = 0.1703704
  pruned tree : test error = 0.1962963

  pruned tree의 테스트오류율이 더 높다.
  